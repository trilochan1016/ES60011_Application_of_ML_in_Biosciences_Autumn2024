{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e19474f4",
   "metadata": {},
   "source": [
    "# Backpropagation Neural Network Implementation using NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15b2129e-68e2-4d77-9c32-37ed819ac084",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the activation function and its derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Mean Squared Error Loss function\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "# R² accuracy calculation (for regression problems)\n",
    "def accuracy(y_true, y_pred):\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    return 1 - (ss_res / ss_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83eb5a3c-f642-4a8a-ba6b-130d64dcfa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Backpropagation Neural Network class\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate):\n",
    "        # Initialize network parameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Initialize weights with random values\n",
    "        self.weights_input_hidden = np.random.randn(input_size, hidden_size)\n",
    "        self.weights_hidden_output = np.random.randn(hidden_size, output_size)\n",
    "        \n",
    "        # Initialize biases with random values\n",
    "        self.bias_hidden = np.random.randn(hidden_size)\n",
    "        self.bias_output = np.random.randn(output_size)\n",
    "\n",
    "    def feedforward(self, X):\n",
    "        # Feedforward through the network\n",
    "        self.hidden_input = np.dot(X, self.weights_input_hidden) + self.bias_hidden\n",
    "        self.hidden_output = sigmoid(self.hidden_input)\n",
    "        \n",
    "        self.final_input = np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output\n",
    "        self.final_output = sigmoid(self.final_input)\n",
    "        \n",
    "        return self.final_output\n",
    "\n",
    "    def backpropagate(self, X, y, output):\n",
    "            # Compute error in the output\n",
    "            output_error = y - output\n",
    "            output_delta = output_error * sigmoid_derivative(output)\n",
    "            \n",
    "            # Compute error in the hidden layer\n",
    "            hidden_error = np.dot(output_delta, self.weights_hidden_output.T)\n",
    "            hidden_delta = hidden_error * sigmoid_derivative(self.hidden_output)\n",
    "            \n",
    "            # Update weights and biases (Gradient Descent)\n",
    "            self.weights_hidden_output += np.dot(self.hidden_output.T, output_delta) * self.learning_rate\n",
    "            self.weights_input_hidden += np.dot(X.T, hidden_delta) * self.learning_rate\n",
    "            \n",
    "            self.bias_output += np.sum(output_delta, axis=0) * self.learning_rate\n",
    "            self.bias_hidden += np.sum(hidden_delta, axis=0) * self.learning_rate\n",
    "\n",
    "    def train(self, X, y, epochs=1000):\n",
    "        for epoch in range(epochs):\n",
    "            output = self.feedforward(X)\n",
    "            self.backpropagate(X, y, output)\n",
    "            if epoch % 100 == 0:\n",
    "                loss = mse_loss(y, output)\n",
    "                print(f'Epoch {epoch}, Loss: {loss}')\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.feedforward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ecd5356-95a5-4e4d-a223-e99c7a335c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Preprocess the housing dataset for training\n",
    "def preprocess_data(data):\n",
    "    # Normalize the input features\n",
    "    X = data[['RM', 'LSTAT', 'PTRATIO']].values\n",
    "    X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "    \n",
    "    # Normalize the target (MEDV)\n",
    "    y = data[['MEDV']].values\n",
    "    y = (y - y.min()) / (y.max() - y.min())\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Load the dataset\n",
    "housing_data = pd.read_csv('housing.csv')\n",
    "X, y = preprocess_data(housing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5248ffa-d8a0-4006-ab07-01221098d9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the number of neurons in the hidden layer:  3\n",
      "Enter the learning rate:  0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-Fold Cross-Validation for Case (a1): Hidden Layer Size = 3, Learning Rate = 0.01\n",
      "Epoch 0, Loss: 0.1392008784218467\n",
      "Epoch 100, Loss: 0.011450801917266926\n",
      "Epoch 200, Loss: 0.010480518019733332\n",
      "Epoch 300, Loss: 0.009998842048308507\n",
      "Epoch 400, Loss: 0.009628150566287014\n",
      "Epoch 500, Loss: 0.009303928649552288\n",
      "Epoch 600, Loss: 0.009011463957115802\n",
      "Epoch 700, Loss: 0.008742649212008178\n",
      "Epoch 800, Loss: 0.00848178332625007\n",
      "Epoch 900, Loss: 0.008214096184964567\n",
      "Fold 1, Loss: 0.0027605115341776034, Accuracy (R²): 0.8028232361550633\n",
      "Epoch 0, Loss: 0.09684638403228671\n",
      "Epoch 100, Loss: 0.009446259701537549\n",
      "Epoch 200, Loss: 0.008374181658019118\n",
      "Epoch 300, Loss: 0.007616741181726217\n",
      "Epoch 400, Loss: 0.0070375260024347274\n",
      "Epoch 500, Loss: 0.006584661948561024\n",
      "Epoch 600, Loss: 0.0062232147119671115\n",
      "Epoch 700, Loss: 0.005934031851729138\n",
      "Epoch 800, Loss: 0.005704359106524796\n",
      "Epoch 900, Loss: 0.005523093682631494\n",
      "Fold 2, Loss: 0.006777544084627674, Accuracy (R²): 0.7650681667648855\n",
      "Epoch 0, Loss: 0.08745115490326304\n",
      "Epoch 100, Loss: 0.00922477349296805\n",
      "Epoch 200, Loss: 0.007716766359307861\n",
      "Epoch 300, Loss: 0.007188965092917003\n",
      "Epoch 400, Loss: 0.006935594583119375\n",
      "Epoch 500, Loss: 0.006797331697652697\n",
      "Epoch 600, Loss: 0.006701224906053534\n",
      "Epoch 700, Loss: 0.006621469589173382\n",
      "Epoch 800, Loss: 0.006549792575686659\n",
      "Epoch 900, Loss: 0.006483070276562518\n",
      "Fold 3, Loss: 0.013805095955880078, Accuracy (R²): 0.5803078213102104\n",
      "Epoch 0, Loss: 0.18462508015647927\n",
      "Epoch 100, Loss: 0.007096274894481301\n",
      "Epoch 200, Loss: 0.0066744617720632055\n",
      "Epoch 300, Loss: 0.006409478950138991\n",
      "Epoch 400, Loss: 0.006194120126819276\n",
      "Epoch 500, Loss: 0.006009044024377097\n",
      "Epoch 600, Loss: 0.005848314721049393\n",
      "Epoch 700, Loss: 0.00570932894382569\n",
      "Epoch 800, Loss: 0.005590354421184606\n",
      "Epoch 900, Loss: 0.005489667758689301\n",
      "Fold 4, Loss: 0.0072117315806882, Accuracy (R²): 0.6948220781271306\n",
      "Epoch 0, Loss: 0.057791094824801\n",
      "Epoch 100, Loss: 0.008441729725109677\n",
      "Epoch 200, Loss: 0.007288804565235378\n",
      "Epoch 300, Loss: 0.006725778028082749\n",
      "Epoch 400, Loss: 0.006305972880101529\n",
      "Epoch 500, Loss: 0.005964365240856239\n",
      "Epoch 600, Loss: 0.00568789261987126\n",
      "Epoch 700, Loss: 0.005470730286863041\n",
      "Epoch 800, Loss: 0.005304819580285548\n",
      "Epoch 900, Loss: 0.005180223050007705\n",
      "Fold 5, Loss: 0.007203347676316113, Accuracy (R²): 0.47429984946542814\n",
      "\n",
      "Average Loss: 0.007551646166337933, Average Accuracy (R²): 0.6634642303645435\n",
      "\n",
      "10-Fold Cross-Validation for Case (a2): Hidden Layer Size = 3, Learning Rate = 0.01\n",
      "Epoch 0, Loss: 0.037700945081539775\n",
      "Epoch 100, Loss: 0.008878331843936103\n",
      "Epoch 200, Loss: 0.007805754019069035\n",
      "Epoch 300, Loss: 0.00726254080176177\n",
      "Epoch 400, Loss: 0.006893156051118507\n",
      "Epoch 500, Loss: 0.006616825237251837\n",
      "Epoch 600, Loss: 0.006400348340958619\n",
      "Epoch 700, Loss: 0.006226444813448227\n",
      "Epoch 800, Loss: 0.006084738613446297\n",
      "Epoch 900, Loss: 0.005968547381522531\n",
      "Fold 1, Loss: 0.0032667806053443445, Accuracy (R²): 0.8269965148208559\n",
      "Epoch 0, Loss: 0.1512389651787691\n",
      "Epoch 100, Loss: 0.01156159035726618\n",
      "Epoch 200, Loss: 0.01010731649468605\n",
      "Epoch 300, Loss: 0.009352812768290072\n",
      "Epoch 400, Loss: 0.008843898270249003\n",
      "Epoch 500, Loss: 0.008468353118890652\n",
      "Epoch 600, Loss: 0.008174557581621436\n",
      "Epoch 700, Loss: 0.007929322385964778\n",
      "Epoch 800, Loss: 0.007708821501954955\n",
      "Epoch 900, Loss: 0.00749797050924044\n",
      "Fold 2, Loss: 0.0027273682569860934, Accuracy (R²): 0.6594497898121325\n",
      "Epoch 0, Loss: 0.07731916250963945\n",
      "Epoch 100, Loss: 0.007588673723360689\n",
      "Epoch 200, Loss: 0.0069183711139312685\n",
      "Epoch 300, Loss: 0.006497505417617646\n",
      "Epoch 400, Loss: 0.006180605268907699\n",
      "Epoch 500, Loss: 0.0059427198739844195\n",
      "Epoch 600, Loss: 0.00576680014572168\n",
      "Epoch 700, Loss: 0.005637485360267448\n",
      "Epoch 800, Loss: 0.005542042619909113\n",
      "Epoch 900, Loss: 0.005470714510843915\n",
      "Fold 3, Loss: 0.004271536673021691, Accuracy (R²): 0.742862487501131\n",
      "Epoch 0, Loss: 0.0625443457590049\n",
      "Epoch 100, Loss: 0.011802761663589305\n",
      "Epoch 200, Loss: 0.009723535474993588\n",
      "Epoch 300, Loss: 0.008240521364292102\n",
      "Epoch 400, Loss: 0.007275390400396936\n",
      "Epoch 500, Loss: 0.006663798023326144\n",
      "Epoch 600, Loss: 0.006223619399555145\n",
      "Epoch 700, Loss: 0.005892350328268155\n",
      "Epoch 800, Loss: 0.005642254208819184\n",
      "Epoch 900, Loss: 0.0054534442589201215\n",
      "Fold 4, Loss: 0.008762616475857011, Accuracy (R²): 0.7213229374817418\n",
      "Epoch 0, Loss: 0.3490039303632292\n",
      "Epoch 100, Loss: 0.00749746707883385\n",
      "Epoch 200, Loss: 0.00693345905641306\n",
      "Epoch 300, Loss: 0.006701288835952787\n",
      "Epoch 400, Loss: 0.006542578706265891\n",
      "Epoch 500, Loss: 0.0064213909277911485\n",
      "Epoch 600, Loss: 0.006322463874019049\n",
      "Epoch 700, Loss: 0.006235695384746295\n",
      "Epoch 800, Loss: 0.006153001797043727\n",
      "Epoch 900, Loss: 0.006067168327899281\n",
      "Fold 5, Loss: 0.011241460730060007, Accuracy (R²): 0.6647008632218168\n",
      "Epoch 0, Loss: 0.05382776698687142\n",
      "Epoch 100, Loss: 0.008574836934255738\n",
      "Epoch 200, Loss: 0.007806653734509254\n",
      "Epoch 300, Loss: 0.007330654312303263\n",
      "Epoch 400, Loss: 0.007000363653075021\n",
      "Epoch 500, Loss: 0.006759758451107578\n",
      "Epoch 600, Loss: 0.006572552684113921\n",
      "Epoch 700, Loss: 0.006417690363493485\n",
      "Epoch 800, Loss: 0.00628364129876261\n",
      "Epoch 900, Loss: 0.006164314167040068\n",
      "Fold 6, Loss: 0.008391614445010671, Accuracy (R²): 0.7350112809014246\n",
      "Epoch 0, Loss: 0.21743380935255296\n",
      "Epoch 100, Loss: 0.010473340583525611\n",
      "Epoch 200, Loss: 0.009148959160634969\n",
      "Epoch 300, Loss: 0.00849805277984989\n",
      "Epoch 400, Loss: 0.008076295207915852\n",
      "Epoch 500, Loss: 0.007744491751531755\n",
      "Epoch 600, Loss: 0.007453888961136705\n",
      "Epoch 700, Loss: 0.007187115998261952\n",
      "Epoch 800, Loss: 0.00693898982892535\n",
      "Epoch 900, Loss: 0.006708800940728784\n",
      "Fold 7, Loss: 0.005119763066123182, Accuracy (R²): 0.570294000539938\n",
      "Epoch 0, Loss: 0.0426229825595121\n",
      "Epoch 100, Loss: 0.00922776235320839\n",
      "Epoch 200, Loss: 0.008223893178496826\n",
      "Epoch 300, Loss: 0.007619136391622118\n",
      "Epoch 400, Loss: 0.007204256573241198\n",
      "Epoch 500, Loss: 0.006879810680812276\n",
      "Epoch 600, Loss: 0.006603100822906905\n",
      "Epoch 700, Loss: 0.006355591215229641\n",
      "Epoch 800, Loss: 0.006130217862926563\n",
      "Epoch 900, Loss: 0.0059261054071539886\n",
      "Fold 8, Loss: 0.010539957344919226, Accuracy (R²): 0.513384109140585\n",
      "Epoch 0, Loss: 0.10751859772547247\n",
      "Epoch 100, Loss: 0.00714207928288755\n",
      "Epoch 200, Loss: 0.006469507664134181\n",
      "Epoch 300, Loss: 0.006128457541687743\n",
      "Epoch 400, Loss: 0.0058936525393309715\n",
      "Epoch 500, Loss: 0.005691225937890318\n",
      "Epoch 600, Loss: 0.005491227889045172\n",
      "Epoch 700, Loss: 0.005290182558169735\n",
      "Epoch 800, Loss: 0.005106739728639352\n",
      "Epoch 900, Loss: 0.004962176324543101\n",
      "Fold 9, Loss: 0.012178448421092988, Accuracy (R²): -0.05953801449637175\n",
      "Epoch 0, Loss: 0.1934070698394447\n",
      "Epoch 100, Loss: 0.008225135614556806\n",
      "Epoch 200, Loss: 0.007517794927380813\n",
      "Epoch 300, Loss: 0.007123733288018202\n",
      "Epoch 400, Loss: 0.006767659029852459\n",
      "Epoch 500, Loss: 0.006446568808147745\n",
      "Epoch 600, Loss: 0.006179356606238019\n",
      "Epoch 700, Loss: 0.005970569091373082\n",
      "Epoch 800, Loss: 0.005813254275029181\n",
      "Epoch 900, Loss: 0.005696965512830681\n",
      "Fold 10, Loss: 0.0050997434636362955, Accuracy (R²): 0.49580510251358456\n",
      "\n",
      "Average Loss: 0.007159928948205152, Average Accuracy (R²): 0.5870289071436838\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the number of neurons in the hidden layer:  4\n",
      "Enter the learning rate:  0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-Fold Cross-Validation for Case (b1): Hidden Layer Size = 4, Learning Rate = 0.001\n",
      "Epoch 0, Loss: 0.059672337306777616\n",
      "Epoch 100, Loss: 0.036296228454533716\n",
      "Epoch 200, Loss: 0.024410389321619312\n",
      "Epoch 300, Loss: 0.018837177036131216\n",
      "Epoch 400, Loss: 0.015798586211378727\n",
      "Epoch 500, Loss: 0.013994725834996346\n",
      "Epoch 600, Loss: 0.012861013696236033\n",
      "Epoch 700, Loss: 0.012109836770041368\n",
      "Epoch 800, Loss: 0.011586631715251473\n",
      "Epoch 900, Loss: 0.011204845618790092\n",
      "Fold 1, Loss: 0.004650058380023308, Accuracy (R²): 0.6678574055166275\n",
      "Epoch 0, Loss: 0.04197117680479531\n",
      "Epoch 100, Loss: 0.01572779751588371\n",
      "Epoch 200, Loss: 0.013065857045445982\n",
      "Epoch 300, Loss: 0.01189210243434245\n",
      "Epoch 400, Loss: 0.011361295264799771\n",
      "Epoch 500, Loss: 0.011083225028331279\n",
      "Epoch 600, Loss: 0.010898775376871369\n",
      "Epoch 700, Loss: 0.010750208449279366\n",
      "Epoch 800, Loss: 0.010617580879812563\n",
      "Epoch 900, Loss: 0.010493958580846062\n",
      "Fold 2, Loss: 0.011480898529428858, Accuracy (R²): 0.6020345268099833\n",
      "Epoch 0, Loss: 0.18407851646708398\n",
      "Epoch 100, Loss: 0.009276410489449844\n",
      "Epoch 200, Loss: 0.00822972439395794\n",
      "Epoch 300, Loss: 0.007598835436906657\n",
      "Epoch 400, Loss: 0.007201339413613553\n",
      "Epoch 500, Loss: 0.006938966346973756\n",
      "Epoch 600, Loss: 0.006754017936392979\n",
      "Epoch 700, Loss: 0.006613678656723246\n",
      "Epoch 800, Loss: 0.006499734360584031\n",
      "Epoch 900, Loss: 0.006402223016283497\n",
      "Fold 3, Loss: 0.01247576632754472, Accuracy (R²): 0.6207211041802453\n",
      "Epoch 0, Loss: 0.21496214234388872\n",
      "Epoch 100, Loss: 0.029635810355639582\n",
      "Epoch 200, Loss: 0.026489761649054044\n",
      "Epoch 300, Loss: 0.024036342242732427\n",
      "Epoch 400, Loss: 0.021962149712214652\n",
      "Epoch 500, Loss: 0.020077574754199596\n",
      "Epoch 600, Loss: 0.018292301794718392\n",
      "Epoch 700, Loss: 0.016633701634332913\n",
      "Epoch 800, Loss: 0.01520303776830428\n",
      "Epoch 900, Loss: 0.014060690431743739\n",
      "Fold 4, Loss: 0.01450794876547349, Accuracy (R²): 0.3860689897636548\n",
      "Epoch 0, Loss: 0.030365687089343465\n",
      "Epoch 100, Loss: 0.014772035983557986\n",
      "Epoch 200, Loss: 0.011926000764352818\n",
      "Epoch 300, Loss: 0.010620480069495336\n",
      "Epoch 400, Loss: 0.010013800589225493\n",
      "Epoch 500, Loss: 0.009698950475679015\n",
      "Epoch 600, Loss: 0.009507098369689415\n",
      "Epoch 700, Loss: 0.009370837150483506\n",
      "Epoch 800, Loss: 0.009262093846481498\n",
      "Epoch 900, Loss: 0.009168298626441614\n",
      "Fold 5, Loss: 0.0114487200432987, Accuracy (R²): 0.16447267011991773\n",
      "\n",
      "Average Loss: 0.010912678409153815, Average Accuracy (R²): 0.4882309392780858\n",
      "\n",
      "10-Fold Cross-Validation for Case (b2): Hidden Layer Size = 4, Learning Rate = 0.001\n",
      "Epoch 0, Loss: 0.15123121500733122\n",
      "Epoch 100, Loss: 0.017248133080015545\n",
      "Epoch 200, Loss: 0.012521467101097241\n",
      "Epoch 300, Loss: 0.010168154060038508\n",
      "Epoch 400, Loss: 0.009033547087739256\n",
      "Epoch 500, Loss: 0.00846798418030911\n",
      "Epoch 600, Loss: 0.008166190599649953\n",
      "Epoch 700, Loss: 0.007988359469551213\n",
      "Epoch 800, Loss: 0.007870314698181289\n",
      "Epoch 900, Loss: 0.007782307003057936\n",
      "Fold 1, Loss: 0.004040259375896395, Accuracy (R²): 0.7860343140539394\n",
      "Epoch 0, Loss: 0.11964458833229298\n",
      "Epoch 100, Loss: 0.03129155442534574\n",
      "Epoch 200, Loss: 0.02119258549766395\n",
      "Epoch 300, Loss: 0.016322917541186775\n",
      "Epoch 400, Loss: 0.013741977639125305\n",
      "Epoch 500, Loss: 0.012206123392081054\n",
      "Epoch 600, Loss: 0.011202210519058682\n",
      "Epoch 700, Loss: 0.010500461174334296\n",
      "Epoch 800, Loss: 0.009987091610409886\n",
      "Epoch 900, Loss: 0.009599509387457952\n",
      "Fold 2, Loss: 0.0036039518799108362, Accuracy (R²): 0.549996020131559\n",
      "Epoch 0, Loss: 0.11310218317765586\n",
      "Epoch 100, Loss: 0.018085875458487496\n",
      "Epoch 200, Loss: 0.01286606454133396\n",
      "Epoch 300, Loss: 0.011200924582523787\n",
      "Epoch 400, Loss: 0.010457691278687545\n",
      "Epoch 500, Loss: 0.010010287932613018\n",
      "Epoch 600, Loss: 0.009680818457540352\n",
      "Epoch 700, Loss: 0.009412758164898845\n",
      "Epoch 800, Loss: 0.009185884193266014\n",
      "Epoch 900, Loss: 0.008991281554450089\n",
      "Fold 3, Loss: 0.00446207216172126, Accuracy (R²): 0.7313926523206442\n",
      "Epoch 0, Loss: 0.20026631147309024\n",
      "Epoch 100, Loss: 0.011173191066452651\n",
      "Epoch 200, Loss: 0.010392546901477761\n",
      "Epoch 300, Loss: 0.009859891812697912\n",
      "Epoch 400, Loss: 0.00945724759660257\n",
      "Epoch 500, Loss: 0.009144251962961283\n",
      "Epoch 600, Loss: 0.008894531105876036\n",
      "Epoch 700, Loss: 0.008690485005602194\n",
      "Epoch 800, Loss: 0.008520130346852986\n",
      "Epoch 900, Loss: 0.008375145710441958\n",
      "Fold 4, Loss: 0.012222524183355366, Accuracy (R²): 0.6112876621542751\n",
      "Epoch 0, Loss: 0.07588703712593751\n",
      "Epoch 100, Loss: 0.02802371090367695\n",
      "Epoch 200, Loss: 0.015362752241754464\n",
      "Epoch 300, Loss: 0.011244637866251508\n",
      "Epoch 400, Loss: 0.009623189489832047\n",
      "Epoch 500, Loss: 0.008863785623878725\n",
      "Epoch 600, Loss: 0.008457676493336843\n",
      "Epoch 700, Loss: 0.008215066924804825\n",
      "Epoch 800, Loss: 0.008054747873582976\n",
      "Epoch 900, Loss: 0.007938507412325015\n",
      "Fold 5, Loss: 0.018534887495031765, Accuracy (R²): 0.44715976627961596\n",
      "Epoch 0, Loss: 0.38366330695055306\n",
      "Epoch 100, Loss: 0.015605526536178032\n",
      "Epoch 200, Loss: 0.011635403185455242\n",
      "Epoch 300, Loss: 0.009849259325615171\n",
      "Epoch 400, Loss: 0.008788446152006383\n",
      "Epoch 500, Loss: 0.008098855124067208\n",
      "Epoch 600, Loss: 0.007620500546761311\n",
      "Epoch 700, Loss: 0.007272806117054013\n",
      "Epoch 800, Loss: 0.007011556704732432\n",
      "Epoch 900, Loss: 0.006810652783823228\n",
      "Fold 6, Loss: 0.004010697693200471, Accuracy (R²): 0.8733509920674802\n",
      "Epoch 0, Loss: 0.0676146616333651\n",
      "Epoch 100, Loss: 0.035522291710464164\n",
      "Epoch 200, Loss: 0.02380721367256915\n",
      "Epoch 300, Loss: 0.01851323700607764\n",
      "Epoch 400, Loss: 0.015684100904931834\n",
      "Epoch 500, Loss: 0.013991404714262322\n",
      "Epoch 600, Loss: 0.012897810023342171\n",
      "Epoch 700, Loss: 0.012151383374469518\n",
      "Epoch 800, Loss: 0.011619937407480208\n",
      "Epoch 900, Loss: 0.011228084905504454\n",
      "Fold 7, Loss: 0.006341569690264816, Accuracy (R²): 0.4677467479048151\n",
      "Epoch 0, Loss: 0.3760097972811095\n",
      "Epoch 100, Loss: 0.3076943767002227\n",
      "Epoch 200, Loss: 0.042096035503305686\n",
      "Epoch 300, Loss: 0.025702344352588564\n",
      "Epoch 400, Loss: 0.01786367580889771\n",
      "Epoch 500, Loss: 0.01405906827305268\n",
      "Epoch 600, Loss: 0.012187795981659377\n",
      "Epoch 700, Loss: 0.011235400133664685\n",
      "Epoch 800, Loss: 0.010725056944979465\n",
      "Epoch 900, Loss: 0.01043233576903371\n",
      "Fold 8, Loss: 0.017080607816165022, Accuracy (R²): 0.21141092730417232\n",
      "Epoch 0, Loss: 0.10481495564502348\n",
      "Epoch 100, Loss: 0.02804977827169642\n",
      "Epoch 200, Loss: 0.020644308269287027\n",
      "Epoch 300, Loss: 0.015992907906798255\n",
      "Epoch 400, Loss: 0.013108662339206304\n",
      "Epoch 500, Loss: 0.011434887825547165\n",
      "Epoch 600, Loss: 0.010491180025254688\n",
      "Epoch 700, Loss: 0.009953843293846047\n",
      "Epoch 800, Loss: 0.009634059945518874\n",
      "Epoch 900, Loss: 0.009429209330770414\n",
      "Fold 9, Loss: 0.018900234085367274, Accuracy (R²): -0.6443405435492617\n",
      "Epoch 0, Loss: 0.09979934049988844\n",
      "Epoch 100, Loss: 0.024075787813067\n",
      "Epoch 200, Loss: 0.0168031936160623\n",
      "Epoch 300, Loss: 0.013703412650985023\n",
      "Epoch 400, Loss: 0.012156124729837526\n",
      "Epoch 500, Loss: 0.011251464694314876\n",
      "Epoch 600, Loss: 0.0106390628456256\n",
      "Epoch 700, Loss: 0.010171721327857939\n",
      "Epoch 800, Loss: 0.009784369829701914\n",
      "Epoch 900, Loss: 0.009447459125512472\n",
      "Fold 10, Loss: 0.009081052313357388, Accuracy (R²): 0.10218616429437266\n",
      "\n",
      "Average Loss: 0.009827785669427058, Average Accuracy (R²): 0.41362247029616117\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the number of neurons in the hidden layer:  5\n",
      "Enter the learning rate:  0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5-Fold Cross-Validation for Case (c1): Hidden Layer Size = 5, Learning Rate = 0.0001\n",
      "Epoch 0, Loss: 0.0749547312558926\n",
      "Epoch 100, Loss: 0.05838009412967923\n",
      "Epoch 200, Loss: 0.042971711865112554\n",
      "Epoch 300, Loss: 0.030728584454315255\n",
      "Epoch 400, Loss: 0.022473363184308343\n",
      "Epoch 500, Loss: 0.01765465442197903\n",
      "Epoch 600, Loss: 0.015122169077497149\n",
      "Epoch 700, Loss: 0.013863537170802197\n",
      "Epoch 800, Loss: 0.013237509391958543\n",
      "Epoch 900, Loss: 0.012904740680135841\n",
      "Fold 1, Loss: 0.004016207769922755, Accuracy (R²): 0.7131318448780987\n",
      "Epoch 0, Loss: 0.05642940979297587\n",
      "Epoch 100, Loss: 0.04481977862092068\n",
      "Epoch 200, Loss: 0.03711821573397541\n",
      "Epoch 300, Loss: 0.032207878982818544\n",
      "Epoch 400, Loss: 0.028846390182576884\n",
      "Epoch 500, Loss: 0.026290056437074202\n",
      "Epoch 600, Loss: 0.02419381500767348\n",
      "Epoch 700, Loss: 0.022408543721526678\n",
      "Epoch 800, Loss: 0.02086409009882582\n",
      "Epoch 900, Loss: 0.019520042423185568\n",
      "Fold 2, Loss: 0.015609020996347023, Accuracy (R²): 0.4589403075968853\n",
      "Epoch 0, Loss: 0.09048331502167299\n",
      "Epoch 100, Loss: 0.08159444394859937\n",
      "Epoch 200, Loss: 0.07057085251129404\n",
      "Epoch 300, Loss: 0.05764382288364938\n",
      "Epoch 400, Loss: 0.04409466678300518\n",
      "Epoch 500, Loss: 0.032197734490138755\n",
      "Epoch 600, Loss: 0.02378586438673535\n",
      "Epoch 700, Loss: 0.01891148742414145\n",
      "Epoch 800, Loss: 0.01641919740908726\n",
      "Epoch 900, Loss: 0.015167290679972467\n",
      "Fold 3, Loss: 0.045909259654932165, Accuracy (R²): -0.39569889758041943\n",
      "Epoch 0, Loss: 0.1296734074943811\n",
      "Epoch 100, Loss: 0.1166547503886823\n",
      "Epoch 200, Loss: 0.1000376109834032\n",
      "Epoch 300, Loss: 0.08031593409297332\n",
      "Epoch 400, Loss: 0.060021658422900824\n",
      "Epoch 500, Loss: 0.04305736855300164\n",
      "Epoch 600, Loss: 0.031726634525923246\n",
      "Epoch 700, Loss: 0.025392880899271164\n",
      "Epoch 800, Loss: 0.022177979707488395\n",
      "Epoch 900, Loss: 0.020556023709708465\n",
      "Fold 4, Loss: 0.011836705114424171, Accuracy (R²): 0.4991076653053662\n",
      "Epoch 0, Loss: 0.16502112685829196\n",
      "Epoch 100, Loss: 0.15970832448176314\n",
      "Epoch 200, Loss: 0.15327479171567945\n",
      "Epoch 300, Loss: 0.14560842928285853\n",
      "Epoch 400, Loss: 0.13676266000977322\n",
      "Epoch 500, Loss: 0.12707633796571777\n",
      "Epoch 600, Loss: 0.1172045971806586\n",
      "Epoch 700, Loss: 0.10793989035763313\n",
      "Epoch 800, Loss: 0.09987948174336814\n",
      "Epoch 900, Loss: 0.09320586902546148\n",
      "Fold 5, Loss: 0.016808818867147482, Accuracy (R²): -0.22670722084135675\n",
      "\n",
      "Average Loss: 0.01883600248055472, Average Accuracy (R²): 0.20975473987171483\n",
      "\n",
      "10-Fold Cross-Validation for Case (c2): Hidden Layer Size = 5, Learning Rate = 0.0001\n",
      "Epoch 0, Loss: 0.10031218130534315\n",
      "Epoch 100, Loss: 0.05626387620829006\n",
      "Epoch 200, Loss: 0.03601047070952425\n",
      "Epoch 300, Loss: 0.027863838596355422\n",
      "Epoch 400, Loss: 0.024382970447165087\n",
      "Epoch 500, Loss: 0.022607283693550796\n",
      "Epoch 600, Loss: 0.021480074141554728\n",
      "Epoch 700, Loss: 0.02062464471077318\n",
      "Epoch 800, Loss: 0.019902692448018025\n",
      "Epoch 900, Loss: 0.019260982306819743\n",
      "Fold 1, Loss: 0.0070546221666874616, Accuracy (R²): 0.6263984733280465\n",
      "Epoch 0, Loss: 0.07450488553077218\n",
      "Epoch 100, Loss: 0.06860011588233512\n",
      "Epoch 200, Loss: 0.06290365648214893\n",
      "Epoch 300, Loss: 0.05747349439105734\n",
      "Epoch 400, Loss: 0.05239188033565887\n",
      "Epoch 500, Loss: 0.04772746253166881\n",
      "Epoch 600, Loss: 0.043515242601189816\n",
      "Epoch 700, Loss: 0.03975504501386775\n",
      "Epoch 800, Loss: 0.03642120441319033\n",
      "Epoch 900, Loss: 0.033474610155224196\n",
      "Fold 2, Loss: 0.005013642610481363, Accuracy (R²): 0.37397634498648413\n",
      "Epoch 0, Loss: 0.03887791544911255\n",
      "Epoch 100, Loss: 0.0366673077247578\n",
      "Epoch 200, Loss: 0.0346691635004654\n",
      "Epoch 300, Loss: 0.03284230372609668\n",
      "Epoch 400, Loss: 0.0311634881424278\n",
      "Epoch 500, Loss: 0.029616467305942203\n",
      "Epoch 600, Loss: 0.02818831601807794\n",
      "Epoch 700, Loss: 0.026868135565544553\n",
      "Epoch 800, Loss: 0.025646514568161027\n",
      "Epoch 900, Loss: 0.024515233089705307\n",
      "Fold 3, Loss: 0.00970339426404084, Accuracy (R²): 0.41587609920103696\n",
      "Epoch 0, Loss: 0.14081046485546442\n",
      "Epoch 100, Loss: 0.07824288583794814\n",
      "Epoch 200, Loss: 0.055309321412277415\n",
      "Epoch 300, Loss: 0.046443231426160005\n",
      "Epoch 400, Loss: 0.041468192060115185\n",
      "Epoch 500, Loss: 0.03772473873504818\n",
      "Epoch 600, Loss: 0.03453471625415615\n",
      "Epoch 700, Loss: 0.03171467907721334\n",
      "Epoch 800, Loss: 0.029204126289895247\n",
      "Epoch 900, Loss: 0.02697150349315902\n",
      "Fold 4, Loss: 0.04310175027789804, Accuracy (R²): -0.3707628526176463\n",
      "Epoch 0, Loss: 0.06677468813218884\n",
      "Epoch 100, Loss: 0.06034072215254412\n",
      "Epoch 200, Loss: 0.05386787966851124\n",
      "Epoch 300, Loss: 0.04760156304464259\n",
      "Epoch 400, Loss: 0.04176538064648637\n",
      "Epoch 500, Loss: 0.03651386784025738\n",
      "Epoch 600, Loss: 0.03191725417724567\n",
      "Epoch 700, Loss: 0.027975663980216697\n",
      "Epoch 800, Loss: 0.02464477703491037\n",
      "Epoch 900, Loss: 0.021858511272845847\n",
      "Fold 5, Loss: 0.04239346775774757, Accuracy (R²): -0.26447029309958925\n",
      "Epoch 0, Loss: 0.09430528801607702\n",
      "Epoch 100, Loss: 0.08327075112190546\n",
      "Epoch 200, Loss: 0.07089511880334812\n",
      "Epoch 300, Loss: 0.05837312324103385\n",
      "Epoch 400, Loss: 0.04737082649045888\n",
      "Epoch 500, Loss: 0.03907540733254318\n",
      "Epoch 600, Loss: 0.033524439581361525\n",
      "Epoch 700, Loss: 0.029966941999343136\n",
      "Epoch 800, Loss: 0.02758532716464863\n",
      "Epoch 900, Loss: 0.025832046028946478\n",
      "Fold 6, Loss: 0.0666138742227367, Accuracy (R²): -1.1035195694639852\n",
      "Epoch 0, Loss: 0.05254667250105814\n",
      "Epoch 100, Loss: 0.04737364474837481\n",
      "Epoch 200, Loss: 0.04383145373939126\n",
      "Epoch 300, Loss: 0.04092106469451199\n",
      "Epoch 400, Loss: 0.03838252640806023\n",
      "Epoch 500, Loss: 0.03612734107752324\n",
      "Epoch 600, Loss: 0.034109397411354166\n",
      "Epoch 700, Loss: 0.03229535105509253\n",
      "Epoch 800, Loss: 0.03065782387683074\n",
      "Epoch 900, Loss: 0.02917364212689515\n",
      "Fold 7, Loss: 0.007736075782653363, Accuracy (R²): 0.3507046843476207\n",
      "Epoch 0, Loss: 0.14045146584234114\n",
      "Epoch 100, Loss: 0.06359504003749426\n",
      "Epoch 200, Loss: 0.03546292328421927\n",
      "Epoch 300, Loss: 0.027008962055064194\n",
      "Epoch 400, Loss: 0.023822733139111776\n",
      "Epoch 500, Loss: 0.022114726369433253\n",
      "Epoch 600, Loss: 0.020902017373721944\n",
      "Epoch 700, Loss: 0.01990713219358098\n",
      "Epoch 800, Loss: 0.019042363182926448\n",
      "Epoch 900, Loss: 0.018274453480274504\n",
      "Fold 8, Loss: 0.015610989634779629, Accuracy (R²): 0.279261255076398\n",
      "Epoch 0, Loss: 0.14265590063519482\n",
      "Epoch 100, Loss: 0.08133858209128186\n",
      "Epoch 200, Loss: 0.051701877828999855\n",
      "Epoch 300, Loss: 0.03917148020300578\n",
      "Epoch 400, Loss: 0.03270858533898127\n",
      "Epoch 500, Loss: 0.028438265451634533\n",
      "Epoch 600, Loss: 0.02518337266887521\n",
      "Epoch 700, Loss: 0.022561725534084084\n",
      "Epoch 800, Loss: 0.020414973911465108\n",
      "Epoch 900, Loss: 0.018649974262145503\n",
      "Fold 9, Loss: 0.03438308457066459, Accuracy (R²): -1.991365065451685\n",
      "Epoch 0, Loss: 0.0485560792882814\n",
      "Epoch 100, Loss: 0.03774173925695569\n",
      "Epoch 200, Loss: 0.030759817579778616\n",
      "Epoch 300, Loss: 0.026642185046284562\n",
      "Epoch 400, Loss: 0.024248676960439343\n",
      "Epoch 500, Loss: 0.02277211604816971\n",
      "Epoch 600, Loss: 0.02175986724907836\n",
      "Epoch 700, Loss: 0.020986746360712648\n",
      "Epoch 800, Loss: 0.02034664590335327\n",
      "Epoch 900, Loss: 0.019789876062981188\n",
      "Fold 10, Loss: 0.006195876121837461, Accuracy (R²): 0.38743406440666384\n",
      "\n",
      "Average Loss: 0.023780677740952706, Average Accuracy (R²): -0.12964668592866554\n"
     ]
    }
   ],
   "source": [
    "# 5-Fold and 10-Fold Cross-validation\n",
    "def k_fold_cross_validation(X, y, k_folds, hidden_size, learning_rate):\n",
    "    fold_size = len(X) // k_folds\n",
    "    fold_results = []\n",
    "    \n",
    "    for i in range(k_folds):\n",
    "        # Split the data into training and validation sets\n",
    "        X_train = np.concatenate((X[:i * fold_size], X[(i + 1) * fold_size:]), axis=0)\n",
    "        y_train = np.concatenate((y[:i * fold_size], y[(i + 1) * fold_size:]), axis=0)\n",
    "        X_val = X[i * fold_size: (i + 1) * fold_size]\n",
    "        y_val = y[i * fold_size: (i + 1) * fold_size]\n",
    "        \n",
    "        # Initialize the neural network\n",
    "        input_size = X.shape[1]\n",
    "        output_size = 1  # Predicting a single output (regression)\n",
    "        nn = NeuralNetwork(input_size, hidden_size, output_size, learning_rate)\n",
    "        \n",
    "        # Train the model\n",
    "        nn.train(X_train, y_train, epochs=1000)\n",
    "        \n",
    "        # Test on the validation set\n",
    "        predictions = nn.predict(X_val)\n",
    "        loss = mse_loss(y_val, predictions)\n",
    "        acc = accuracy(y_val, predictions)\n",
    "        print(f\"Fold {i+1}, Loss: {loss}, Accuracy (R²): {acc}\")\n",
    "        \n",
    "        fold_results.append((loss, acc))\n",
    "    \n",
    "    # Average results across all folds\n",
    "    avg_loss = np.mean([result[0] for result in fold_results])\n",
    "    avg_acc = np.mean([result[1] for result in fold_results])\n",
    "    print(f\"\\nAverage Loss: {avg_loss}, Average Accuracy (R²): {avg_acc}\")\n",
    "\n",
    "# User inputs for the neural network configuration\n",
    "hidden_layer_size = int(input(\"Enter the number of neurons in the hidden layer: \"))\n",
    "learning_rate = float(input(\"Enter the learning rate: \"))\n",
    "output_layer_size = 1  # Since we're doing regression\n",
    "\n",
    "# Running the experiments with different configurations\n",
    "print(f\"5-Fold Cross-Validation for Case (a1): Hidden Layer Size = {hidden_layer_size}, Learning Rate = {learning_rate}\")\n",
    "k_fold_cross_validation(X, y, k_folds=5, hidden_size=hidden_layer_size, learning_rate=learning_rate)\n",
    "\n",
    "print(f\"\\n10-Fold Cross-Validation for Case (a2): Hidden Layer Size = {hidden_layer_size}, Learning Rate = {learning_rate}\")\n",
    "k_fold_cross_validation(X, y, k_folds=10, hidden_size=hidden_layer_size, learning_rate=learning_rate)\n",
    "\n",
    "# User inputs for the neural network configuration\n",
    "hidden_layer_size = int(input(\"Enter the number of neurons in the hidden layer: \"))\n",
    "learning_rate = float(input(\"Enter the learning rate: \"))\n",
    "output_layer_size = 1  # Since we're doing regression\n",
    "\n",
    "# Running the experiments with different configurations\n",
    "print(f\"5-Fold Cross-Validation for Case (b1): Hidden Layer Size = {hidden_layer_size}, Learning Rate = {learning_rate}\")\n",
    "k_fold_cross_validation(X, y, k_folds=5, hidden_size=hidden_layer_size, learning_rate=learning_rate)\n",
    "\n",
    "print(f\"\\n10-Fold Cross-Validation for Case (b2): Hidden Layer Size = {hidden_layer_size}, Learning Rate = {learning_rate}\")\n",
    "k_fold_cross_validation(X, y, k_folds=10, hidden_size=hidden_layer_size, learning_rate=learning_rate)\n",
    "\n",
    "# User inputs for the neural network configuration\n",
    "hidden_layer_size = int(input(\"Enter the number of neurons in the hidden layer: \"))\n",
    "learning_rate = float(input(\"Enter the learning rate: \"))\n",
    "output_layer_size = 1  # Since we're doing regression\n",
    "\n",
    "# Running the experiments with different configurations\n",
    "print(f\"\\n5-Fold Cross-Validation for Case (c1): Hidden Layer Size = {hidden_layer_size}, Learning Rate = {learning_rate}\")\n",
    "k_fold_cross_validation(X, y, k_folds=5, hidden_size=hidden_layer_size, learning_rate=learning_rate)\n",
    "\n",
    "print(f\"\\n10-Fold Cross-Validation for Case (c2): Hidden Layer Size = {hidden_layer_size}, Learning Rate = {learning_rate}\")\n",
    "k_fold_cross_validation(X, y, k_folds=10, hidden_size=hidden_layer_size, learning_rate=learning_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
